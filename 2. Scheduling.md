# Scheduling

In the properties of a pod, there is an attribute spec.nodeName, that shows which node the pod is running on. Is none is defined, the pod is candidate for scheduling. If there are no scheduler, the pod will stay in pending state. This attribute can be set manually during creation only.

If a pod is already created, it can still be manually assigned to a node using a binding object, mimicking what the scheduler actually does

```yaml
apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: node02 #node that we want to assign the pod to.
```
This must then be converted to json format, end send as body to the pod's binding api.

## Labels & Selectors

Labels are key value properties that the user can define however he want under the metadata attribute of an item.

Can be used as an argument to filter objects, like ```kubectl get pods –selector app=app1[,tier=tier1…]```

In a ReplicaSet, we use the attribute ```spec.selector.matchLabels``` to match on specific labels

## Taints & Toleration

Taint is set on a node, and prevents pods from being schedule on it if the matching toleration is not defined on the pod.

To apply a taint: ```kubectl taint nodes node1 key1=value1:NoSchedule```, to remove it: ```kubectl taint nodes node1 key1=value1:NoSchedule-```

```NoSchedule``` is a Taint Effect. Other possible values are ```PreferNoSchedule```, that will schedule the pod if no other nodes are available, and ```NoExecute```, that will act as ```NoSchedule```, but also evicts the preexisting pods on nodes before the taint was applied.

Toleration is added on the pod in its specification, under the spec attribute

```yaml
tolerations:
- key: "key1"
  operator: "Equal" #default operator
  value: "value1"
  effect: "NoSchedule"
```

Operator can also be ```Exists```, in this case ```value``` should not be specified

> :warning: Master Node has a NoSchedule Taint by default so no pods are scheduled on it.

## Node Selector & Node Affinity

To Deploy a pod only on specific node, an option is to use ```nodeSelector```, an attribute under pod's spec attribute, where we add key value pair that match the desired nodes. We can also use the command ```kubectl label nodes <name> K=V[,K2=V2...]```

Node affinity provides advanced capability to select nodes.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In #Other options are: NotIn, Exists, DoesNotExist, Gt and Lt
            values:
            - antarctica-east1
            - antarctica-west1
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1 #Preferred can have a weight from 1 to 100. Highest are prioritized
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: registry.k8s.io/pause:2.0
```
> RequiredDuringExecution doesn't exist yet at the time of writing

## Resources requirements

**Request** define the minimum resources for a pod to run, and **Limits** the maximum it is allowed to use from the node. A good practice is not to define a limit in most case, so a pod use as much resource as it wants, and other pods still get their minimum requests.

**cpu** and **memory** are the most common resources to specify. If a pod try to exceed it cpu limit, it will throttle, however it can exceed its memory limit, but will be terminated with an Out of Memory error after a while.

If a limit it set without a request, then request will inherit the limit definition.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "1"
```

- 1 CPU is theequivalent of 1 AWS vCPU, 1 GCP or Azure core, or 1 Hyperthread
- 1 CPU can also be written as 1000m, minimum value is 1m.
- Acceptable units for memory are G, M, K, Gi, Mi, Ki, or nothing (bytes)

### Limit Range

Define min/max values for resources at a namespace level. Only affects pods created after it's definition

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default: # this section defines default limits
      cpu: 500m
    defaultRequest: # this section defines default requests
      cpu: 500m
    max: # max limit
      cpu: "1"
    min: # min request
      cpu: 100m
    type: Container
```

### Resource Quota

A ResourceQuota object define at a namespace level a hard limit

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
```

### Edit pods Resource requirements

The only editable specs of an existing pod are :
- spec.containers[*].image
- spec.initContainers[*].image
- spec.activeDeadlineSeconds
- spec.tolerations

So in order to change a readonly param, options are :
- Make a change on vim, it will be saved on a temp file, delete the pod, run a create command on the temp file
- ```kubectl get pod webapp -o yaml > my-new-pod.yaml```, edit the spec, delete the pod, run a create command on the file
- Edit the deployment, who will kill & recreate the pod itself

## Daemon Set

Similar to ReplicaSet (definition is identical, except for kind attribute), it deploys multiple instances of pods, but precisely one copy on each node. If a new node is added, a replica of the pod will be added on it.

Useful for monitoring solutions, log viewer, networking.

## Static pods

A Kubelet is able to read a pod definition file (and only pods) from a directory with the argument ```--pod-manifest-path```, or its ```--config``` argument can refer another yaml that will have a ```staticPodPath``` attribute.

In this context, we need to use docker/crictl/nerdctl ```ps``` command to view content, as ```kubectl``` is not available on a single pod. ```ps -aux | grep kubelet``` helps identify the config file.

Static pods can be identified with their name: they appens the name of the node they are linked to.

Created this way, the pods can still be visible from a master node, but are readonly. To delete them, we must edit the manifest file.

It is useful to create a master node (that's what kubeadm actually does)

To kill a static pod on a node, use ```kubectl get nodes -o wide``` to get node's ip, then ssh into it, and go on ```/var/lib/kubelet/config.yaml```

## Multiple schedulers

We initially have 'default-scheduler', but can add more to schedule with custom rules, or replace the default. In order to do this, we create a KubeSchedulerConfiguration, and a Pod

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler
leaderElection: # if several copies of scheduler are running on different master nodes for HA, only one can be active
  leaderElect: true
  resourceNamespace: kube-system
  resourceName: lock-object-my-scheduler
plugins: # this section is explained in the "Configuring Scheduler Profile" section, at the end of this page
  score:
    disabled:
    - name: TaintToleration
    enabled:
    - name: MyCustomPluginA
- schedulerName: my-scheduler-2
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --config=/etc/kubernetes/my-scheduler-config.yaml #if defined, do not use the end of the commands
    - --leader-elect=true
    - --scheduler-name=my-customer-scheduler
    - --lock-object-name=my-custom-scheduler
    image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
    name: kube-scheduler
```

A pod can request a specific scheduler to plan it with the attribute ```spec.schedulerName```

- ```kubectl get events -o wide``` shows events, including which scheduler picked a pod
- ```kubectl logs my-customer-scheduler``` shows scheduler logs

## Configuring Scheduler Profile

Pods can have the ```spec.PriorityClassName``` attribute. a PriorityClass object has a value attribute. The higher is place on the front of the **Scheduling Queue**. Then come the **filtering phase**, where nodes that cannot receive the pod are out. Then comes the **Scoring phase**, that compute the free available space after putting the pod. Highest wins. Finally, **Binding Phase** link the pod and the node.

There are many steps between these phases (prefilter, postfilter, prescore, reserve, permit, brebind, postbind..), and many plugin that can span across steps, and be used several times. Common plugins are:
- PrioritySort (scheduling)
- NodeResourceFit (filtering, scoring)
- NodeName & NodeUnschedulable (filtering)
- ImageLocality (scoring)
- DefaultBinder (Binding)

Custom plugin can be enabled in the KubeSchedulerConfiguration, default plugin can also be disabled there.

## References

- https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md
- https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/
- https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
- https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work

