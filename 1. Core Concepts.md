# Core Concepts of Kubernetes

> :warning: if unsure of a command, use `kubectl -h`. You can also use this for a subcommand you are trying to use, like `kubectl get -h`

## Cluster Architecture

### Worker Node
Host apps as containers using Docker or other container runtime engines like containerD or Rocket.

A worker node contains:
  - pods: the apps in their containers
  - Kubelet: listens for instructions about what to do on the node.
  - Kube Proxy: ensures necessary rules are in place to allow communication between nodes.

### Master Node
Manages the cluster. Plans, schedules, and monitors nodes (control plane component).

Components include:
  - **Etcd:** Key-Value store for logs and container states.
  - **Kube-scheduler:** Identifies nodes to place containers based on resource requirements, node capacity, and policies.
  - **Kube-apiserver:** Manages all operations in the cluster.
  - **Controller Manager:** Includes node controller for adding new nodes and replication controller for managing replica count, and many other controllers in the kube-controller-manager.


## Docker & containerD

### Container Runtime Interface (CRI)
Allows any vendor to work as a container runtime for K8s if they follow the Open Container Initiative (OCI).

OCI is made of 2 things:
  - imagespec: how an image should be build
  - runtimespec: how any container runtime should be develop

Docker is more than juste a container runtime. It's daemon that manage this is **ContainerD**. In the past, K8s supported Docker via Dockershim, up to K8s v1.24
### CLI
  - ctr is ContainerD's CLI, use for debug, but not so good. **nerdctl** has access to newer features, and is use for **general purposes**
  - **crictl** is a CLI from the K8s community, is used from K8s perspective and works across different container runtimes, **used to debug**

| docker cli | crictl            | Description                                                          | Unsupported Features                |
|------------|-------------------|----------------------------------------------------------------------|-------------------------------------|
| attach     | attach            | Attach to a running container                                        | --detach-keys, --sig-proxy          |
| exec       | exec              | Run a command in a running container                                 | --privileged, --user, --detach-keys |
| images     | images            | List images                                                          |                                     |
| info       | info              | Display system-wide information                                      |                                     |
| inspect    | inspect, inspecti | Return low-level information on a container, image or task           |                                     |
| logs       | logs              | Fetch the logs of a container                                        | --details                           |
| ps         | ps                | List containers                                                      |                                     |
| stats      | stats             | Display a live stream of container(s) resource usage statistics      | Column: NET/BLOCK I/O, PIDs         |
| version    | version           | Show the runtime (Docker, ContainerD, or others) version information |                                     |

## Etcd
Key-Value database storing information about the cluster (nodes, pods, configs, secrets, etc).

Example commands:
  - `etcdctl set key1 value1` (if using v2)
  - `etcdctl put key1 value1` (if using v3)
  - `etcdctl get key1`
  - `etcdctl get / â€“prefix -keys-only` (from etcd-master pod)
  - `etcdctl --help`
  - `etcdctl --version`
### Setup
  - Cluster from scratch: Download binaries and configure etcd as a service.
  - Cluster from kubeadm: Deploy etcd server as a pod.
### High Availability (HA)
  - Multiple master nodes require multiple instances of ETCD.
  - Configuration using `initial-cluster` parameter to list all instances.

## Kube-apiserver
Receive request, validate authentification, validates requests, retrieves data from etcd, and updates it. It's the only component to interact directly with etcd

To view its option, it depends of how it was created:
  - via Kubeadm: kube-apiserver-master is deployed as a pod. `cat /etc/kubernetes/manifests/kube-apiserver.yaml`
  - `cat /etc/systemd/system/kube-apiserver.service` otherwise
  - `ps -aux | grep kube-apiserver` to list process & effective options

It has an option `-etcd-servers` to list the etcd servers

## Controller-manager
The Kubernetes Controller Manager is a crucial component responsible for managing various controllers that regulate the state of the cluster. Controllers ensure that the desired state of the system matches the actual state by reconciling any differences.
### Node Controller

Checks the status of each node in the cluster every 5 seconds via the API server. If a node becomes unreachable for more than 40 seconds, it marks the node as unreachable. After a default timeout of 5 minutes, it removes any pods assigned to the unhealthy node and provisions them on healthy nodes, if part of a replica set.

### Replication Controller

Ensures the specified number of pod replicas are running at any given time. If a pod dies, the replication controller automatically creates another one to maintain the desired replica count.

### Other Controllers

Kubernetes offers numerous other controllers, all housed within the kube-controller-manager binary. These controllers cover various aspects of cluster management. By default, all controllers are enabled, but customization options are available for specific use cases.

To view its option, it depends of how it was created:
  - via Kubeadm: kube-controller-manager-master is deployed as a pod. `cat /etc/kubernetes/manifests/kube-controller-manager.yaml`
  - `cat /etc/systemd/system/kube-controller-manager.service` otherwise
  - `ps -aux | grep kube-controller-manager` to list process & effective options

## Kube scheduler
The Kubernetes Kube Scheduler is responsible for making optimal decisions about which nodes to place newly created pods on within the cluster. It considers various factors such as resource requirements, node capacity, and user-defined policies to ensure efficient workload distribution, then instructs the Kubelet on the selected node to run the pod.

Configuration and Setup

- **Manifest File Location:** For a Kubernetes cluster setup using Kubeadm: `cat /etc/kubernetes/manifests/kube-scheduler.yaml`
- **Process Information:** `ps -aux | grep kube-scheduler`
 
## Kubelet
Manages worker nodes by registering them, creating pods, and monitoring nodes & pods.

kubeadm does not deploy kubelets. It has to be download by binary using package manager, or curl.

## Kube proxy
Manages pod network and ensures communication between pods on different nodes. It runs on each node to look for new services, and when one is created, makes appropriate rules on every nodes to forward traffic to the service node. It creates an ip table rule to forward traffic.

## Pods
Contains the docker container, and is in worker nodes. It can have a helper container with it, that it can refer to as localhost, and share the same storage.
Managed by Kubectl commands like :
- `kubectl run nginx --image nginx`
- `kubectl get pods`
- `kubectl delete pod <name1> [<name2>...]`
- `kubectl delete pod -l K=V` to delete based on labels
- `kubectl describe pod`
- `kubectl apply -f <file.yaml>`

YAML used for defining pod configurations:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx
    image: nginx
```

## Replication controller & ReplicaSet
Aimed to provide HA, load balancing & scaling, ReplicaSet is a replacement of ReplicationController. The main difference is that it is able to monitor pods that it didn't create. Both spans across nodes

```yaml
apiVersion: apps/v1 #just v1 for ReplicationController
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  replicas: 3 #how many replicas should be running
  selector: #attribute doesn't exist for ReplicationController
    matchLabels:
      tier: frontend #must match labels of the pods to monitor
  template: #how to create the pods, or recreate it in case one goes down
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
```
Just as pods, and most K8s objects, the following commands can be used:
- `kubectl get replicaset`
- `kubectl describe replicaset <name>`
- `kubectl delete replicaset <name> [<name2>...]` this also delete the underlying pods
- `kubectl edit -f <filename>.yaml`

Specific commands:
- `kubectl scale --replicas=<number> -f <filename>.yaml`
- `kubectl scale --replicas=<number> replicaset <name>` this doesn't update the yaml file when modifying

## Deployment
Deployment is higher in the hierarchy than ReplicaSet. It allow rolling update, undo changes, pause & resume

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

## Kubernetes Services
Used for making frontend available to end-users, communication between pods, and connecting to external resources.

K8s nodes has an ip address, and pod has internal IP. Services spread across all nodes and provide internal or external access.

Port of a node is known as **nodeport**, and is define in a range from 30000 to 32767. Port of the service is known as **port**. Port of the pod is **targetport**.

If a targetport is not filled, it will be assumed to be equal to the port value. If nodeport is not filled, it will be automatically allocated.

Service connects to pods with the spec.selector attribute, matching KV to the pods metadata.labels

Types include:
- ClusterIP: only accessible within the cluster. Default type of service. One is automatically created at launch.
- NodePort: accessible with IP/port, but to a specific node
- LoadBalancer: works with some cloud providers native load balancer, to define an address

Command example:
- `kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml` use pods labels as selector
- `kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml` assume selector is app=redis

> :warning: `dry-run=client` means that the service won't be launch when the command is executed. `-o yaml` means it will output a yaml file. This is often used to generate configuration quickly, then edit the yaml.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort #default ClusterIP
  selector:
    app.kubernetes.io/name: MyApp
  ports:
    - port: 80 # `targetPort` is set to the same value as the `port` field by default.
      targetPort: 80
      nodePort: 30007 #optional, control plane will allocate from range 30000-32767 if empty
```

## Namespaces 
Namespaces are used for organizing resources and setting policies. At first, we are in default namespace, and K8s also creates `kube-system` with a set of pods & services for internal purposes, as well as `kube-public`, for resources that should be available to all users.

Namespaces have their policices, and resources quota. A good idea is to have a namespace matching each environment.

When a service is created, a DNS is created automatically. To reach something from another namespace, use `<targetName>.<namespaceName>.<subdomain>.<domain>`, by default it will be `<targetName>.<namespace>.svc.cluster.local`

- For commands on another namespace, simply add `--namespace=<name>` to the command.
- Namespace can be defined with a yaml file, but it's way easier with `kubectl create namespace <name>`
- To change default namespace, use `kubectl config set-context $(kubectl config current-context) â€“namespace=<name>`
- To get a list of all pods across all namespaces, `kubectl get pods --all-namespaces`

## Tips (Declarative vs Imperative)
In Kubernetes, managing resources can be approached in two ways: imperative and declarative.

- **Imperative:** In imperative management, users specify explicit instructions on how to achieve a desired state. For example, they might issue commands to create, update, or delete resources directly. While this approach provides granular control and immediate feedback, it can be more error-prone and less scalable, especially in complex environments.

- **Declarative:** In declarative management, users define the desired state of the system without specifying the exact steps to achieve it. They describe the desired configuration of resources in YAML manifests or JSON files and apply them to the cluster using tools like kubectl. Kubernetes then automatically reconciles the current state with the desired state, making necessary changes to achieve alignment. This approach promotes consistency, repeatability, and scalability, as it focuses on the desired outcome rather than the specific steps to reach it. It's also better as all the information are shown in the yaml file, can be defined in git and is not lost in the CLI.

However, during the exam, imperative is good to know as it is way faster to implement. Best practice is to use `kubectl create ..... --dry-run=client -o yaml > myFile.yaml`, edit the `myFile.yaml` on vim to get the exact configuration you want, and then run `kubectl apply -f myFile.yaml`
